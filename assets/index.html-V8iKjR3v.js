import{_ as d,c as u,b as p,e as o,d as r,w as i,a as n,r as e,o as s}from"./app-DNksp9_M.js";const m="/assets/image-20241229201334489-Dp99hwrU.png",f="/assets/image-20241229201452774-wyiZXoXH.png",b={};function H(a,l){const t=e("font");return s(),u("div",null,[l[64]||(l[64]=p("p",null,"复习建议：",-1)),l[65]||(l[65]=p("p",null,"1.按照复习提纲，严格复习重点笔记",-1)),l[66]||(l[66]=p("p",null,"2.考前3天，一定要反复手写HQL语句！！！，手写课上讲过的HQL语句",-1)),l[67]||(l[67]=p("p",null,"3.论述题，分要点写，字迹工整，思路清晰 1。。。。。 2。。。。。 3。。。。。。",-1)),l[68]||(l[68]=p("h2",{id:"填空题-2-10-20分",tabindex:"-1"},[p("a",{class:"header-anchor",href:"#填空题-2-10-20分"},[p("span",null,"填空题（ 2*10=20分）")])],-1)),l[69]||(l[69]=p("blockquote",null,[p("p",null,"要点：基础概念，基本理论"),p("p",null,"特点：好得分")],-1)),p("p",null,[l[4]||(l[4]=o("Hive:由Facebook开源用于解决海量")),r(t,{color:"red"},{default:i(()=>l[0]||(l[0]=[o("结构化")])),_:1}),l[5]||(l[5]=o("日志的数据统计工具。 Hive是基于Hadoop的一个")),r(t,{color:"red"},{default:i(()=>l[1]||(l[1]=[o("数据仓库工具")])),_:1}),l[6]||(l[6]=o("，可以将")),r(t,{color:"red"},{default:i(()=>l[2]||(l[2]=[o("结构化的数据文件映射为一张表")])),_:1}),l[7]||(l[7]=o("，并提供")),r(t,{color:"red"},{default:i(()=>l[3]||(l[3]=[o("类SQL")])),_:1}),l[8]||(l[8]=o("查询功能。"))]),l[70]||(l[70]=n("<p>Yarn（Yet Another Resource Negotiator）是Hadoop 2.0中的<mark>资源管理器</mark></p><p>Sqoop是一款开源的数据<mark>导入导出工具</mark></p><p>Hive是基于Hadoop的一个<mark>分布式数据仓库工具</mark></p><p>HDFS（Hadoop Distributed Filesystem）是一个易于扩展的<mark>分布式文件系统</mark></p><p>Hive 处理的数据存储在 <mark>HDFS</mark></p><p>Hive 分析数据底层的实现是 <mark>MapReduce</mark></p><p>执行程序运行在 <mark>Yarn</mark> 上</p><p><strong>Hive优点</strong></p>",8)),p("p",null,[l[10]||(l[10]=o("（1）操作接口采用类 SQL 语法，")),r(t,{color:"red"},{default:i(()=>l[9]||(l[9]=[o("提供快速开发的能力")])),_:1}),l[11]||(l[11]=o("（简单、容易上手）。"))]),p("p",null,[l[13]||(l[13]=o("（2）")),r(t,{color:"red"},{default:i(()=>l[12]||(l[12]=[o("避免了去写 MapReduce")])),_:1}),l[14]||(l[14]=o("，减少开发人员的学习成本。"))]),p("p",null,[l[16]||(l[16]=o("（3）Hive 的执行延迟比较高，因此 ")),r(t,{color:"red"},{default:i(()=>l[15]||(l[15]=[o("Hive 常用于数据分析，对实时性要求不高的场合。 ")])),_:1})]),p("p",null,[l[18]||(l[18]=o("（4）Hive ")),r(t,{color:"red"},{default:i(()=>l[17]||(l[17]=[o("优势在于处理大数据")])),_:1}),l[19]||(l[19]=o("，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。"))]),p("p",null,[l[21]||(l[21]=o("（5）")),r(t,{color:"red"},{default:i(()=>l[20]||(l[20]=[o("Hive 支持用户自定义函数")])),_:1}),l[22]||(l[22]=o("，用户可以根据自己的需求来实现自己的函数。无法实现。"))]),l[71]||(l[71]=p("p",null,[p("strong",null,"Hive缺点")],-1)),l[72]||(l[72]=p("p",null,"Hive 的 HQL 表达能力有限",-1)),p("p",null,[l[24]||(l[24]=o("（1）")),r(t,{color:"red"},{default:i(()=>l[23]||(l[23]=[o("迭代式算法无法表达")])),_:1})]),p("p",null,[l[26]||(l[26]=o("（2）数据挖掘方面不擅长，由于")),r(t,{color:"red"},{default:i(()=>l[25]||(l[25]=[o(" MapReduce 数据处理流程的限制，效率更高的算法却无法实现。")])),_:1})]),l[73]||(l[73]=p("p",null,"Hive 的效率比较低",-1)),p("p",null,[l[28]||(l[28]=o("（1）Hive 自动生成的 MapReduce 作业，通常情况下")),r(t,{color:"red"},{default:i(()=>l[27]||(l[27]=[o("不够智能化")])),_:1})]),p("p",null,[l[30]||(l[30]=o("（2）Hive 调优比较困难，")),r(t,{color:"red"},{default:i(()=>l[29]||(l[29]=[o("粒度较粗")])),_:1})]),p("p",null,[l[32]||(l[32]=o("际工作中分区表常常被运用于按照**")),r(t,{color:"red"},{default:i(()=>l[31]||(l[31]=[o("某一维度")])),_:1}),l[33]||(l[33]=o("**进行统计分析的场景下，数据被按照某一个日期、年月日等等，将一个大的文件切分成一个个小文件，分而治之，这样处理起来性能就会有显著提升。"))]),l[74]||(l[74]=n('<h2 id="简答题-5-6-30分" tabindex="-1"><a class="header-anchor" href="#简答题-5-6-30分"><span>简答题（5*6=30分）</span></a></h2><blockquote><p>要点：</p><ol><li><p>内部表、外部表、分区表、分桶表</p></li><li><p>Hive数据表的创建、数据导入和导出</p></li></ol><p><code>load data</code></p><p><code>location</code></p><p><code>insert</code></p><ol start="3"><li><p>HDFS shell 基本命令：增删改HDFS文件和文件夹</p></li><li><p>数据表、数据库的基本操作：修改、增加和删除</p></li><li><p>Hive中的排序</p></li></ol></blockquote>',2)),p("ol",null,[p("li",null,[l[35]||(l[35]=p("p",null,"分区表一般在数据量比较大，且有明确的分区字段时使用，这样用分区字段作为查询条件查询效率会比较高。Hive分区分为静态分区和动态分区。",-1)),r(t,{color:"red"},{default:i(()=>l[34]||(l[34]=[o("1、为什么出现分区表？")])),_:1}),l[36]||(l[36]=p("p",null,"假设有海量的数据保存在hdfs的某一个hive表名对应的目录下，使用hive进行操作的时候，会搜索这个目录下的所有的文件，非常耗时。(目录下的文件数据都会被加载到hive表中）",-1)),l[37]||(l[37]=p("p",null,[o("如果我们知道这些数据的某些特征，然后使用Hive进行操作的时候，就可以在"),p("mark",null,"where子句种对这些特征进行过滤"),o("，那么对数据的操作就会在符合条件的子目录下进行，其他不符合条件的目录下的内容就不会被读取。")],-1)),l[38]||(l[38]=p("p",null,"在数据量非常大的时候，这样节省大量的时间，这种把表中的数据分散到子目录下的方式就是分区表。",-1))]),p("li",null,[p("p",null,[l[40]||(l[40]=o("分桶表：桶表是对某一列数据进行哈希取值以将数据打散，然后放到不同文件中存储。在Hive分区表中，分区中的数据量过于庞大时，建议使用桶。 在分桶时，对指定字段的值进行hash运算得到hash值，并使用hash值除以桶的个数做取余运算得到的值进行分桶，保证每个桶中有数据但每个桶中的数据不一定相等。做hash运算时，hash函数的选择取决于分桶字段的数据类型，分桶后的查询效率比分区后的")),r(t,{color:"red"},{default:i(()=>l[39]||(l[39]=[o("查询效率更高")])),_:1}),l[41]||(l[41]=o("。"))])])]),l[75]||(l[75]=n('<h3 id="可能考" tabindex="-1"><a class="header-anchor" href="#可能考"><span>可能考</span></a></h3><p>HiveQL和SQL的区别：举例说明Hie查询语言(HiveQL)和标准SQL之间的至少三个差异。</p><h2 id="编程题-6-5-30分" tabindex="-1"><a class="header-anchor" href="#编程题-6-5-30分"><span>编程题（6*5=30分）</span></a></h2><blockquote><p>要点：</p><ol><li><p>Hive数据库和数据表的基本操作：修改、增加和删除</p></li><li><p>Hive基础操作：分组、排序、关键字段的执行顺序</p></li><li><p>窗口排序函数 + lateral view + explode</p></li></ol></blockquote><div class="hint-container caution"><p class="hint-container-title">警告</p><p>注意细节：</p><p>1.按照题目要求写HQL语句，比如最终输出结果有2个字段：job和job_min_sal</p><p>2.HDFS路径严格遵循题干要求</p></div><p>SELECT 字段1,字段2... FROM 表名</p><ol start="2"><li><p>分组、排序、关键字段执行顺序</p><p>SELECT 字段1,字段2... FROM 表名</p><p>​ WHERE 约束条件(表中原始字段)</p><p>​ GROUP BY field(表中原始字段)</p><p>​ HAVING 过滤条件</p><p>​ ORDER BY field(排序字段)</p><p>​ LIMIT 限制条数</p></li></ol><p>1.找到表:from</p><p>2.拿着where指定的约束条件，去文件/表中取出一条条记录</p><p>3.将取出的一条条记录进行分组group by，如果没有group by，则整体作为一组</p><p>4.将分组的结果进行having过滤</p><p>5.执行select</p><p>6.去重DISTINCT</p><p>7.将结果按条件排序：order by</p><p>8.限制结果的显示条数</p><h2 id="论述题-10-2-20分" tabindex="-1"><a class="header-anchor" href="#论述题-10-2-20分"><span>论述题（10*2=20分）</span></a></h2><blockquote><p>要点：</p><p>1.性能调优</p><p>2.大数据压缩格式</p></blockquote><p><strong>大数据压缩</strong></p><p>压缩:使用压缩技术来把数据“减少”的过程</p><p>解压缩将压缩过后的数据转换成原始数据的过程</p><p><strong>为什么要使用压缩？</strong></p><p>（1）节省文件的存储空间</p><p>（2）加速数据在网络中的传输速度</p>',23)),r(t,{color:"red"},{default:i(()=>l[42]||(l[42]=[o("**就是时间和空间的取舍和博弈**")])),_:1}),r(t,{color:"red"},{default:i(()=>l[43]||(l[43]=[o("两种常用的压缩技术：")])),_:1}),p("p",null,[l[45]||(l[45]=o("1.")),r(t,{color:"red"},{default:i(()=>l[44]||(l[44]=[o("无损压缩")])),_:1}),l[46]||(l[46]=o("(不会有任何数据的丢失，即原始数据和压缩后的数据在解压后一样)通常情况下，不允许有任何数据的丢失；"))]),p("p",null,[l[48]||(l[48]=o("2.")),r(t,{color:"red"},{default:i(()=>l[47]||(l[47]=[o("有损压缩")])),_:1}),l[49]||(l[49]=o(" 例如应用在图片，视频，音频(高清，超清视频等，允许数据存在小部分的丢失)"))]),p("p",null,[l[51]||(l[51]=p("mark",null,"在大数据领域通常使用的是",-1)),r(t,{color:"red"},{default:i(()=>l[50]||(l[50]=[p("strong",null,"无损压缩",-1)])),_:1})]),p("p",null,[l[53]||(l[53]=p("strong",null,"压缩使用场景",-1)),l[54]||(l[54]=o("：")),r(t,{color:"red"},{default:i(()=>l[52]||(l[52]=[o("输入文件的压缩，中间结果的压缩，输出文件结果的压缩 ")])),_:1})]),p("p",null,[l[56]||(l[56]=o("1、Compressing input files:对应的是mapreduce中的")),r(t,{color:"red"},{default:i(()=>l[55]||(l[55]=[o("map端的输入")])),_:1}),l[57]||(l[57]=o("，能够自动解压被压缩的文件，不需要写代码去解压被压缩的文件"))]),p("p",null,[l[59]||(l[59]=o("2、Compressing output files:对应的是mapreduce中的 ")),r(t,{color:"red"},{default:i(()=>l[58]||(l[58]=[o("reduce输出")])),_:1})]),p("p",null,[l[61]||(l[61]=o("3、Compressing map output ：对应的是mapreduce中的")),r(t,{color:"red"},{default:i(()=>l[60]||(l[60]=[o("map端的输出")])),_:1}),l[62]||(l[62]=o("，首先写到本地的磁盘空间中去，然后通过shuffle将数据传输到reduce端去处理。"))]),l[76]||(l[76]=p("img",{src:m,alt:"image-20241229201334489",style:{zoom:"67%"}},null,-1)),l[77]||(l[77]=p("img",{src:f,alt:"image-20241229201452774",style:{zoom:"50%"}},null,-1)),l[78]||(l[78]=p("p",null,[p("strong",null,"大数据领域的压缩注意事项")],-1)),r(t,{color:"red"},{default:i(()=>l[63]||(l[63]=[o(" 是否选用压缩要根据集群的实际情况考虑。")])),_:1}),l[79]||(l[79]=p("p",null,"1.例如集群的存储空间不够，但是CPU的利用率不高，此时建议使用压缩，因为压缩后，能够缓解磁盘的压力，同时能够提高CPU的利用率；",-1)),l[80]||(l[80]=p("p",null,"2.如果集群空间足够大，但CPU的利用率已经很高了，例如已经达到90%了，那么这种情况下，就不要选用压缩，如果选择压缩，会导致CPU的压力过大。",-1)),l[81]||(l[81]=p("p",null,"小结：压缩比越高，压缩和解压的时间越长。压缩比和压缩速度是成反比的。",-1)),l[82]||(l[82]=p("p",null,"压缩比： bzip2>gzip>lzo",-1)),l[83]||(l[83]=p("p",null,"压缩速度：lzo>gzip>bzip2",-1))])}const v=d(b,[["render",H],["__file","index.html.vue"]]),k=JSON.parse('{"path":"/article/0bdmx9a6/","title":"Hive期末复习","lang":"zh-CN","frontmatter":{"title":"Hive期末复习","createTime":"2024/12/27 15:33:52","permalink":"/article/0bdmx9a6/","tags":["Hive"]},"headers":[],"readingTime":{"minutes":6.9,"words":2070},"git":{"updatedTime":1735490917000,"contributors":[{"name":"wuchen","username":"wuchen","email":"3180349973@qq.com","commits":3,"avatar":"https://avatars.githubusercontent.com/wuchen?v=4","url":"https://github.com/wuchen"}],"changelog":[{"hash":"b181ee8aa0c4103f8ca90216414edaa5e8e6ba3b","date":1735490917000,"email":"3180349973@qq.com","author":"wuchen","message":"docs: add hadoop,hive qimofuxi","commitUrl":"https://github.com/ChGoh7/ChGoh7.github.io/tree/docs/commit/b181ee8aa0c4103f8ca90216414edaa5e8e6ba3b"},{"hash":"828b31e7288d4b6cc09049641786ac989e067ec6","date":1735404182000,"email":"3180349973@qq.com","author":"wuchen","message":"update qimofuxi","commitUrl":"https://github.com/ChGoh7/ChGoh7.github.io/tree/docs/commit/828b31e7288d4b6cc09049641786ac989e067ec6"},{"hash":"835c4162426d05c7d9f94a7d8be87922c23c31e5","date":1735324699000,"email":"3180349973@qq.com","author":"wuchen","message":"add exams revises","commitUrl":"https://github.com/ChGoh7/ChGoh7.github.io/tree/docs/commit/835c4162426d05c7d9f94a7d8be87922c23c31e5"}]},"filePathRelative":"DOING/Hive期末复习.md","categoryList":[{"id":"233681","sort":10064,"name":"DOING"}],"bulletin":false}');export{v as comp,k as data};
